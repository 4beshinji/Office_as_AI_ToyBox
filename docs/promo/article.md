# LLMが「脳」になったオフィス — 共生型環境管理システム SOMS の設計思想

## リード: ビル管理システムの限界

「室温が26度を超えたら冷房をONにする。」

現代のビル管理システム（BMS）は、本質的にこのようなIF-THENルールの集合体である。温度、湿度、CO2——センサーが閾値を超えたら、あらかじめ決められたアクションを実行する。決定論的で、予測可能で、そして限界がある。

では、こんな状況はどうだろうか。

- ホワイトボードが汚れている。次の会議まであと30分。
- 3人が会議室で2時間座りっぱなしだ。そろそろ休憩を促したい。
- 天気予報は豪雨。窓が2枚開いている。片方はスマートアクチュエータ付き、もう片方は手動式。

これらの課題には、文脈の理解と、物理的な行動が必要だ。従来のBMSには、どちらの能力もない。

SOMS（Symbiotic Office Management System）は、この限界を突破するために設計された自律型オフィス環境管理システムである。

## 「共生」の意味: 人間を自動化するのではなく、協働する

SOMSの名前に含まれる「Symbiotic（共生）」は、飾りではない。このシステムの設計思想の核心を表している。

スマートホームのAPIで操作できないタスクは、世の中に無数に存在する。窓を閉める。コーヒーを淹れる。ホワイトボードを拭く。散らかった机を片付ける。ロボットアームなしに、AIはどうやってこれらの物理タスクを解決するのか？

SOMSの答えはシンプルだ。**人間を「高度な汎用アクチュエータ」として、経済的インセンティブで動かす**。

LLMが状況を判断し、必要なタスクを生成し、報酬（「最適化承認スコア」と名付けたゲーム的ポイント）を提示して人間に依頼する。人間は自由意志でタスクを受諾し、完了し、スコアを蓄積する。AIが人間を置き換えるのではなく、AIと人間が互いの得意分野を活かして協働する。これが「共生」の意味である。

## 有機体としてのアーキテクチャ

SOMSのシステム全体は、一つの有機体として設計されている。

**脳**はLLM（大規模言語モデル）だ。ローカルのGPUサーバー上で動作するQwen2.5 14Bモデルが、30秒ごとにオフィスの状態を「考え」、必要に応じて行動する。ReAct（Think→Act→Observe）と呼ばれる認知ループで、最大5回の反復を行い、状況に応じてツールを選択する。

**神経系**はMQTT（Message Queuing Telemetry Transport）ブローカーだ。軽量なパブリッシュ/サブスクライブ型プロトコルが、全コンポーネント間の通信を担う。ここに独自の工夫がある。AIとツールの標準インターフェースであるMCP（Model Context Protocol）を、通常のHTTPではなくMQTT上に実装した。これにより、ESP32のような省電力マイコンでもAIのツール呼び出しに直接応答できるようになった。

**感覚器**はセンサーとカメラだ。BME680（温湿度・気圧・ガス）、MH-Z19C（CO2）がオフィスの環境データをリアルタイムに計測し、YOLOv11がカメラ映像から在室人数、活動レベル、姿勢を解析する。生のピクセルデータは構造化されたJSONに変換され、LLMが理解できる形で「脳」に届けられる。

**声**はVOICEVOX。日本語音声合成エンジンが、AIの判断を自然な日本語で人間に伝える。「少し体を動かしてみませんか？」——caringなトーンで。「CO2濃度が上がっています」——alertなトーンで。

そして**手足**は、ESP32エッジデバイスと、ダッシュボード経由で参加する人間だ。

## 3つのシナリオ

### 嵐のプロトコル

気圧センサーが急低下を検知し、天気APIが「15分後に豪雨」を返す。LLMは即座に全窓の状態を確認する。窓3にはスマートアクチュエータがあるので、MCP over MQTTでコマンドを送り自動閉鎖。窓5は手動式——LLMは報酬を最大額の5000スコアに引き上げ、緊急タスクを生成する。「【緊急】窓5を至急閉めてください！」alertトーンの音声が流れ、ダッシュボードに金色の報酬バッジが輝くタスクカードが表示される。近くにいた社員が受諾し、窓を閉める。自動化と人間協働のハイブリッドで、嵐に備える。

### 座りっぱなし検知

YOLOv11の姿勢推定モデルが、4層の時間バッファを使って長時間の姿勢を追跡する。60秒の生データから10分の中期傾向、1時間の長期追跡、4時間の超長期記録まで、段階的に時間解像度を粗くしながらデータを保持する。30分間「mostly_static」が続くと、健康助言イベントが発火する。ここでLLMが選ぶのは`create_task`（タスク生成）ではなく`speak`（音声アナウンス）だ。「少し体を動かしてみませんか？」——これは命令ではなく、助言。そしてクールダウンは1時間。煩わしさを避ける設計上の配慮がある。

### センサー改竄検知

30秒以内に温度が5度以上変化する。通常の環境では物理的にありえない速度だ。WorldModelのイベント検知がこれを捕捉し、LLMに報告する。LLMの反応は「おやおや、センサーに何か起きたかな？」——humorousトーンの音声アナウンス。深刻なアラームではなく、ユーモアで指摘する。センサーの前で息を吹きかけて遊んでいる人がいるかもしれないのだから。

## データ主権の思想

SOMSの全処理はオンプレミスで完結する。映像、音声、センサーデータは一切クラウドに送信しない。

この設計思想の数値的なインパクトは劇的だ。1日にカメラとセンサーが生成する生データは約50GB。SOMSはこれをローカルで処理し、構造化されたイベントログ（約500MB）に圧縮し、外部に送信可能なのは1時間ごとの集約統計（約1MB/日）のみ。**50,000:1**の圧縮率。都市が外部に送る必要があるデータは、生成されるデータの0.002%に過ぎない。

カメラ映像はRAM上でのみ処理され、ディスクに保存されない。YOLOが「3人在室、活動レベル中程度」というJSONを出力した瞬間に、元のピクセルデータは破棄される。Core Hub（SOMSが動作するサーバー）を物理的に撤去すれば、その拠点の全生データが消失する。これは物理的なデータ主権の保証だ。

## 都市への展望

SOMSは単体のオフィス管理システムではない。「Core Hub」アーキテクチャの最初の実装として設計されている。

Core Hubとは、ローカルLLMとGPUを持つ自律的な処理拠点のことだ。同じアーキテクチャで、接続するセンサーとシステムプロンプトを変えるだけで、オフィス、農場、店舗、公共施設に展開できる。各Core Hubは独立して思考し、行動し、ネットワークが切断されても自律動作を継続する。

都市規模では、複数のCore Hubの集約統計をCity Data Hubが統合分析する。オフィス街のCO2パターン、商業施設の人流、住宅街の生活リズム——単一拠点では見えない「都市の呼吸パターン」が、1時間平均のCO2値という最小限のデータから浮かび上がる。

## 技術スタック

- **LLM**: Qwen2.5 14B (Ollama, AMD ROCm)
- **Vision**: YOLOv11 (物体検出 + 姿勢推定)
- **Backend**: Python 3.11, FastAPI, SQLAlchemy async
- **Frontend**: React 19, TypeScript, Vite 7, Tailwind CSS 4
- **Voice**: VOICEVOX (日本語音声合成)
- **Messaging**: MQTT (Mosquitto) + MCP (JSON-RPC 2.0)
- **Edge**: ESP32 (MicroPython / PlatformIO C++)
- **GPU**: AMD RX 9700 (RDNA4, 32GB VRAM)

Node-RED、LangChain、Kubernetesは使わない。Python + MQTTによる純粋なイベント駆動アーキテクチャ。LLM自身がシステムの論理構造を理解できる透明性を確保するための選択だ。

## まとめ

SOMSが問いかけるのは、「AIはオフィスで何ができるか」ではない。「AIと人間は、物理空間でどう協働できるか」という問いだ。

従来のBMSが「自動化」の延長線上にあるのに対し、SOMSは「自律性」——AIが自ら考え、人間と交渉し、協働する——という新しいパラダイムを提案する。そしてその全処理は、GPU1台の上で、クラウドに頼ることなく完結する。

LLMが「脳」になったオフィスは、すでに動いている。
