# 都市にAIを実装する — 分散型ローカルLLMによる空間知能の実証

## スマートシティが抱える構造的な問題

「スマートシティ」は10年以上にわたって語られてきた。しかし現在の実装には、繰り返し指摘される共通の問題がある。

センサーが温度を測り、カメラが人を映す。そのデータはクラウドに集約され、外部企業のサーバーで処理される。自治体は自身のデータにアクセスするためにAPI料金を支払う。ネットワークが切断されると全機能が停止する。カメラ映像の保管・アクセス権限は外部企業に帰属し、データ主権は都市の手にない。

問題の構造は明確だ。データが生まれる場所（建物）と、処理される場所（クラウド）が物理的に分離している。都市が自身のデータを、自身の計算資源で、自身の論理で処理する——この形が実現すれば、都市は外部依存なしに自律的に状況を把握し、判断を行える。ローカルGPUによるLLM推論が実用水準に達した現在、この構造を組み替える技術的な条件は整っている。

## Core Hub: 建物にAIの処理拠点を配置する

この構想の基盤となるアーキテクチャを Core Hub と呼ぶ。

Core Hub は、ローカルLLM（大規模言語モデル）とGPUを持つ自律的な処理拠点だ。1台のGPUサーバーが建物全体の「脳」として機能し、センサーデータの解釈、カメラ映像の分析、状況に応じた行動判断を行う。全処理はローカルで完結し、外部にデータを送信しない。

```
                      ┌──────────────────┐
                      │   City Data Hub  │ 集約統計のみ受信
                      └────────┬─────────┘
                               │ ~1MB/Hub/日
              ┌────────────────┼────────────────┐
              │                │                │
        ┌─────┴─────┐   ┌─────┴─────┐   ┌─────┴─────┐
        │  Office   │   │   Farm    │   │  Public   │
        │  Hub      │   │   Hub     │   │  Facility │
        │  LLM+GPU  │   │  LLM+GPU  │   │  LLM+GPU  │
        └───────────┘   └───────────┘   └───────────┘
```

接続するセンサーとシステムプロンプト（LLMへの行動指針）を変えるだけで、同じアーキテクチャがオフィス、農場、店舗、公共施設に展開できる。各Core Hubは独立して思考し、行動し、ネットワークが切断されても自律動作を継続する。

都市規模では、複数のCore Hubの集約統計をCity Data Hubが統合分析する。各Hubから外部に出るデータは1時間集約の統計値のみで、1日あたり約1MB。50GBの生データから1MBの構造化情報への圧縮比は50,000:1。この圧縮比がデータ主権の技術的な裏付けとなる。

## SOMS: 最初のCore Hub実装

SOMS（Symbiotic Office Management System）は、Core Hubアーキテクチャの最初の実装だ。1つのオフィスを対象に、分散型ローカルAIが空間を自律管理できることを検証する Phase 0 の実験場にあたる。

システムは一つの有機体になぞらえて設計されている。

脳はLLM。ローカルGPU上で動作するQwen2.5 14Bが、30秒ごとにオフィスの状態を「考え」、必要に応じて行動する。ReAct（Think→Act→Observe）認知ループで最大5回の反復を行い、5つのツールから状況に応じた行動を選択する。3層の安全機構——重複検知フィルタ、レート制限、30分間の行動履歴追跡——が暴走を防ぐ。正常時は「何もしない」で1反復で終了する設計だ。

神経系はMQTTブローカー。軽量なパブリッシュ/サブスクライブ型プロトコルが全コンポーネント間の通信を担う。AIとツールの標準インターフェースであるMCP（Model Context Protocol）をMQTT上に実装した。ESP32のような省電力マイコンでもAIのツール呼び出しに直接応答できる。HTTPの重いオーバーヘッドを避け、LLMの推論（秒単位）とデバイスの応答（ミリ秒〜分）の時間差をブローカーが吸収する。

感覚器はセンサーとカメラ。BME680（温湿度・気圧・ガス）、MH-Z19C（CO2）がオフィスの環境データをリアルタイムに計測する。YOLOv11がカメラ映像から在室人数、活動レベル、姿勢を解析する。姿勢分析は4層の時間バッファで追跡する——30ミリ秒の生データから10分の中期傾向、1時間の長期追跡、4時間の超長期記録まで段階的に保持する。映像はRAM上でのみ処理し、ディスクに保存しない。

声はVOICEVOX。日本語音声合成エンジンがAIの判断を自然な日本語で人間に伝える。タスクの依頼には説得力ある口調で、健康助言にはやさしい口調で、緊急時には警戒的な口調で。タスクが無視された際の音声は最大100パターンをバックグラウンドで事前生成し、即座に再生できるよう備蓄している。

## SensorSwarm: 建物全体を安価に高密度カバーする

都市のAI化は末端のセンシング密度に依存する。WiFi接続のセンサーノードだけでは、AP帯域やチャネル干渉がボトルネックになり、カバレッジに限界がある。

SOMSのエッジ層は SensorSwarm という2層アーキテクチャを採用している。Hub（ESP32、WiFi+MQTT接続）が各ゾーンに1台。その配下に複数のLeafノードがESP-NOW、UART、I2C、BLEの4種類のトランスポートで接続される。

```
[Core Hub / Brain]
       │ MQTT
  ┌────┴────┐
  │ SwarmHub│ (WiFi + MQTT)
  └──┬──┬──┬┘
     │  │  │  ESP-NOW / UART / I2C / BLE
   [L] [L] [L]  Leaf nodes (センサーのみ、WiFi不要)
```

LeafノードはWiFiスタックを持たないため消費電力が低く、バッテリー駆動が実用的になる。Hub-Leaf間の通信は独自バイナリプロトコル（5〜245バイト、XORチェックサム）で、デバイスIDはドット表記（`swarm_hub_01.leaf_env_01`）で階層構造を表現する。

HubがLeafのデータを集約してMQTTに発行すれば、Brain（LLM）からはHub配下の各Leafが独立したセンサーとして見える。WiFiの密度制約を受けずに、安価なLeafノードで建物全体を高密度にカバーできる。

## 物理タスクの解決: AIと人間の経済的な協働

都市のAI化には避けられない問題がある。AIはAPIで操作できない物理タスクをどう解決するのか。

窓を閉める。コーヒーを淹れる。ホワイトボードを拭く。ロボットアームなしに、AIはどうやって物理世界に働きかけるか。

SOMSの設計は、人間を「高度な汎用アクチュエータ」と見なし、経済的インセンティブで協働する形を取る。LLMが状況を判断し、タスクを生成し、報酬を提示する。人間は自由意志でタスクを受諾し、完了し、クレジットを蓄積する。

経済システムは複式簿記で実装されている。システムウォレット（ID=0）からの発行、タスク報酬（500〜5000ポイント）、デバイスXP（運用貢献度に応じた動的乗数1.0x〜3.0x）、P2P送金。デフレ機構として手数料5%焼却とデマレッジ2%/日を導入し、ポイントの希少性を維持する。

ダッシュボードはアカウント不要のキオスク型で、タスク表示と報酬バッジ、音声告知を行う。個人の残高管理はモバイルPWAウォレットアプリで行う。QRコードスキャンによる報酬受取、取引履歴の閲覧、ユーザー間送金——すべてスマートフォンから操作できる。

この設計はCore Hubの汎用コンポーネントとして機能する。農場Hubなら「収穫タスク」、店舗Hubなら「品出しタスク」、公共施設Hubなら「点検タスク」——AIが判断し、人間が遂行し、経済が回る。

## 3つのシナリオ

### 嵐のプロトコル

気圧センサーが急低下を検知する。LLMは全窓の状態を確認する。窓3にはスマートアクチュエータがあるため、MCP over MQTTでコマンドを送り自動閉鎖。窓5は手動式——LLMは報酬を最大額の5000ポイントに引き上げ、緊急タスクを生成する。alertトーンの音声が流れ、ダッシュボードに報酬バッジ付きタスクカードが表示される。近くにいた人が受諾し、窓を閉める。自動化と人間協働のハイブリッドで嵐に備える。

### 座りっぱなし検知

YOLOv11の姿勢推定モデルが、4層の時間バッファで長時間の姿勢を追跡する。30分間「mostly_static」が続くと、健康助言イベントが発火する。LLMが選ぶのは`create_task`ではなく`speak`だ。「少し体を動かしてみませんか？」——命令ではなく助言。1時間のクールダウンが煩わしさを防ぐ。

### センサー改竄検知

30秒以内に温度が5度以上変化する。物理的にありえない速度だ。WorldModelのイベント検知がこれを捕捉してLLMに報告する。LLMの反応は「おやおや、センサーに何か起きたかな？」——humorousトーンの音声アナウンス。深刻なアラームではなくユーモアで指摘する。2段階のタスク重複検知が、同じ問題への二重タスク発行を防ぐ。

## データ主権: 50,000:1

SOMSの全処理はオンプレミスで完結する。映像、音声、センサーデータは一切クラウドに送信しない。

| 層 | データ量 (1 Core Hub/日) | 外部送信 |
|---|---|---|
| 生信号 (映像 + センサー) | ~50 GB | 不可 |
| 構造化イベント (Event Store) | ~500 MB | Hub内保存 |
| City Hub への集約統計 | ~1 MB | 統計値のみ |

カメラ映像はRAM上でのみ処理される。YOLOが「3人在室、活動レベル中程度」というJSONを出力した瞬間に、元のピクセルデータは破棄される。Core Hubを物理的に撤去すれば、その拠点の全生データが消失する。

GDPR、個人情報保護法へのコンプライアンスとして最も確実な方法: データを送信しないこと。

## 都市の呼吸パターン: 1拠点の環境監視が都市知能になる

Core Hubアーキテクチャの価値は、複数拠点が連携する段階で顕著になる。

Phase 0（SOMS単体）では、オフィスのCO2が9:00に急上昇し、12:00に低下し、13:00に再上昇し、18:00に低下する——人数変動との強い相関が観測される。単一拠点の環境モニタリングとして有用だが、それ以上のことは見えない。

複数のCore Hubが稼働するPhase 2では状況が変わる:

```
Hub-A (オフィス街): CO2ピーク 9:00, 13:00
Hub-B (商業施設):   CO2ピーク 11:00, 15:00, 19:00
Hub-C (住宅街):     CO2ピーク 7:00, 20:00
```

City Data Hubが全Hubの集約統計を時空間分析すると、人の流れが可視化される:

> 住宅街(朝) → オフィス街(日中) → 商業施設(夕方) → 住宅街(夜)

大気質の悪化パターン検出、都市計画（換気設備配置、緑地計画）への入力データとしての活用が視野に入る。

この洞察を得るために外部に送ったデータ: 各Hubの1時間平均CO2値のみ（数バイト/Hub/時間）。

各建物が自律的にデータを処理し、最小限の構造化情報だけを共有する。その結果として、単一拠点では見えなかった都市規模のパターンが浮かび上がる。クラウドに50GBを送る必要はない。1MBで十分だ。

## 技術スタック

- **LLM**: Qwen2.5 14B (Ollama, AMD ROCm)
- **Vision**: YOLOv11 (物体検出 + 姿勢推定, 4層時間バッファ)
- **Backend**: Python 3.11, FastAPI, SQLAlchemy async, PostgreSQL 16 (asyncpg)
- **Frontend**: React 19, TypeScript, Vite 7, Tailwind CSS 4
- **Voice**: VOICEVOX (日本語音声合成, 事前生成拒否ストック)
- **Economy**: 複式簿記台帳, デマレッジ, デバイスXP, モバイルPWAウォレット
- **Messaging**: MQTT (Mosquitto) + MCP (JSON-RPC 2.0)
- **Edge**: ESP32 MicroPython + SensorSwarm (Hub-Leaf, ESP-NOW/UART/I2C/BLE)
- **Infra**: Docker Compose (11サービス), nginx, 仮想テスト環境
- **GPU**: AMD RX 9700 (RDNA4)

Node-RED、LangChain、Kubernetesは使わない。Python + MQTTによる純粋なイベント駆動アーキテクチャ。LLM自身がシステムのソースコードを理解できる透明性を確保するための選択だ。

## Phase 0 の達成状況

- ReAct認知ループ: 5ツール, 最大5反復, 3層安全機構 (重複検知/レート制限/行動履歴)
- WorldModel: 指数減衰加重平均によるセンサーフュージョン, イベント検知 (CO2閾値/温度急変/長時間座位)
- MCP over MQTT: JSON-RPC 2.0, ESP32デバイスとの実通信
- Perception: YOLOv11物体検出 + 姿勢推定, 4層活動分析, カメラ自動検出
- SensorSwarm: Hub-Leaf 2層ネットワーク, 4種トランスポート, バイナリプロトコル
- Dashboard: React 19 キオスクUI, 音声統合, 2段階タスク重複検知
- Voice: VOICEVOX日本語音声, LLMテキスト生成, 拒否ストック事前生成 (最大100)
- Wallet: 複式簿記, デマレッジ, 5%焼却, デバイスXP, 供給量追跡
- Wallet PWA: モバイルアプリ (残高/QRスキャン/送金/履歴)
- 仮想環境: Mock LLM + Virtual Edge + Virtual Camera による完全シミュレーション
- テスト: E2E統合テスト (7シナリオ) + 個別テストスイート

## まとめ

SOMSが検証するのは「AIはオフィスで何ができるか」ではなく、「分散型ローカルAIは都市の情報化をどう変えるか」という問いだ。

従来のスマートシティが「データをクラウドに送る」ことを情報化と呼んでいたのに対し、Core Hubアーキテクチャは「データが生まれた場所で処理する」という構造を提案する。各建物が自律的にデータを処理し、最小限の構造化データだけを共有する。その集約から都市規模のパターンが浮かび上がる。

GPU1台。クラウド不要。50,000:1のデータ圧縮。

1つのオフィスから始まる、都市をAI化するためのアーキテクチャは、すでに動いている。
